{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS182 SP22 HW4 student.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KAFAIMAK/cs182_hw3_student/blob/main/CS182_SP22_HW4_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer and Masked Autoencoder\n",
        "\n",
        "In this assignment, you will be implementing [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) and [Masked Autoencoder (MAE)](https://arxiv.org/abs/2111.06377)."
      ],
      "metadata": {
        "id": "ohhBQGEKZ9z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "Sr_tSqTXc0VE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We recommend working on Colab with GPU enabled since this assignment needs a fair amount of compute.\n",
        "In Colab, we can enforce using GPU by clicking `Runtime -> Change Runtime Type -> Hardware accelerator` and selecting `GPU`.\n",
        "The dependencies will be installed once the notebooks are excuted.\n",
        "\n",
        "You should make a copy of this notebook to your Google Drive otherwise the outputs will not be saved.\n",
        "Once the folder is copied, you can start working by clicking a Jupyter Notebook and openning it in Colab."
      ],
      "metadata": {
        "id": "M-f8SLhqa585"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi # Confirm GPU is enabled"
      ],
      "metadata": {
        "id": "nI_3cNKMGL2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install einops"
      ],
      "metadata": {
        "id": "9hFl82FA59le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imL8NunGXV5f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import einops\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive to save models and logs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hHjvJvZhkv-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: change ```root_folder``` to the folder of this notebook in your google drive"
      ],
      "metadata": {
        "id": "X1wxozGQcVUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = \"/content/drive/MyDrive/cs182_hw4_colab/\"\n",
        "os.makedirs(root_folder, exist_ok=True)\n",
        "os.chdir(root_folder)"
      ],
      "metadata": {
        "id": "eRrKGY0ycIS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Testing Data\n",
        "!wget -O '/content/autograder_student.pt' 'https://cs182sp22.github.io/assets/data/autograder_student.pt'\n",
        "!wget -O '/content/test_reference.pt' 'https://cs182sp22.github.io/assets/data/test_reference.pt'\n",
        "\n",
        "test_data = torch.load('/content/test_reference.pt')\n",
        "auto_grader_data = torch.load('/content/autograder_student.pt')\n",
        "auto_grader_data['output'] = {}"
      ],
      "metadata": {
        "id": "nl3t1xKoWee7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utilities for Testing\n",
        "def save_auto_grader_data():\n",
        "    torch.save(\n",
        "        {'output': auto_grader_data['output']},\n",
        "        'autograder.pt'\n",
        "    )\n",
        "\n",
        "def rel_error(x, y):\n",
        "    return torch.max(\n",
        "        torch.abs(x - y)\n",
        "        / (torch.maximum(torch.tensor(1e-8), torch.abs(x) + torch.abs(y)))\n",
        "    ).item()\n",
        "\n",
        "def check_error(name, x, y, tol=1e-3):\n",
        "    error = rel_error(x, y)\n",
        "    if error > tol:\n",
        "        print(f'The relative error for {name} is {error}, should be smaller than {tol}')\n",
        "    else:\n",
        "        print(f'The relative error for {name} is {error}')\n",
        "\n",
        "def check_acc(acc, threshold):\n",
        "    if acc < threshold:\n",
        "        print(f'The accuracy {acc} should >= threshold accuracy {threshold}')\n",
        "    else:\n",
        "        print(f'The accuracy {acc} is better than threshold accuracy {threshold}')"
      ],
      "metadata": {
        "id": "Uv1eceRfsIHZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformer\n",
        "The first part of this notebook is implementing Vision Transformer (ViT) and training it on CIFAR dataset."
      ],
      "metadata": {
        "id": "-Ii0DgIQdsmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image patchify and unpatchify\n",
        "\n",
        "In ViT, an image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard Transformer encoder. The architecture can be seen in the following figure.\n",
        "![vit](https://github.com/google-research/vision_transformer/blob/main/vit_figure.png?raw=true)\n",
        "\n",
        "To get started with implementing ViT, you need to implement splitting image batch into fixed-size patches batch in ```patchify``` and combining patches batch into the original image batch in ```unpatchify```.\n",
        "\n",
        "We strongly recommend using [einops](https://github.com/arogozhnikov/einops) for flexible tensor operations, you can check out its [tutorial](https://einops.rocks/1-einops-basics/). "
      ],
      "metadata": {
        "id": "MquneePhckk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def patchify(images, patch_size=4):\n",
        "    \"\"\"Splitting images into patches.\n",
        "    Args:\n",
        "        images: Input tensor with size (batch, channels, height, width)\n",
        "            We can assume that image is square where height == width.\n",
        "    Returns:\n",
        "        A batch of image patches with size (\n",
        "          batch, (height / patch_size) * (width / patch_size), \n",
        "        channels * patch_size * patch_size)\n",
        "    \"\"\"\n",
        "    # BEGIN YOUR CODE\n",
        "    raise NotImplementedError\n",
        "    # END YOUR CODE\n",
        "\n",
        "def unpatchify(patches, patch_size=4):\n",
        "    \"\"\"Combining patches into images.\n",
        "    Args:\n",
        "        patches: Input tensor with size (\n",
        "        batch, (height / patch_size) * (width / patch_size), \n",
        "        channels * patch_size * patch_size)\n",
        "    Returns:\n",
        "        A batch of images with size (batch, channels, height, width)\n",
        "    \"\"\"\n",
        "    # BEGIN YOUR CODE\n",
        "    raise NotImplementedError\n",
        "    # END YOUR CODE"
      ],
      "metadata": {
        "id": "hS2qs7nD5iyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your implementation\n",
        "x = test_data['input']['patchify']\n",
        "y = test_data['output']['patchify']\n",
        "check_error('patchify', patchify(x), y)\n",
        "\n",
        "x = auto_grader_data['input']['patchify']\n",
        "auto_grader_data['output']['patchify'] = patchify(x)\n",
        "save_auto_grader_data()\n",
        "\n",
        "\n",
        "x = test_data['input']['unpatchify']\n",
        "y = test_data['output']['unpatchify']\n",
        "check_error('unpatchify', unpatchify(x), y)\n",
        "\n",
        "x = auto_grader_data['input']['unpatchify']\n",
        "auto_grader_data['output']['unpatchify'] = unpatchify(x)\n",
        "\n",
        "save_auto_grader_data()"
      ],
      "metadata": {
        "id": "plMY_EoNZ4ip",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT model\n",
        "You need to use the given Transformer encoder ```Transformer``` to implement ViT in ```ViT``` and ```ClassificationViT```."
      ],
      "metadata": {
        "id": "sFlVruRYhBlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Transformer Encoder \n",
        "    Args:\n",
        "        embedding_dim: dimension of embedding\n",
        "        n_heads: number of attention heads\n",
        "        n_layers: number of attention layers\n",
        "        feedforward_dim: hidden dimension of MLP layer\n",
        "    Returns:\n",
        "        Transformer embedding of input\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim=256, n_heads=4, n_layers=4, feedforward_dim=1024):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.feedforward_dim = feedforward_dim\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=embedding_dim,\n",
        "                nhead=self.n_heads,\n",
        "                dim_feedforward=self.feedforward_dim,\n",
        "                activation=F.gelu,\n",
        "                batch_first=True,\n",
        "                dropout=0.0,\n",
        "            ),\n",
        "            num_layers=n_layers,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transformer(x)\n",
        "\n",
        "class ClassificationViT(nn.Module):\n",
        "    \"\"\"Vision transformer for classfication\n",
        "    Args:\n",
        "        n_classes: number of classes \n",
        "        embedding_dim: dimension of embedding\n",
        "        patch_size: image patch size\n",
        "        num_patches: number of image patches\n",
        "    Returns:\n",
        "        Logits of classfication\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes, embedding_dim=256, patch_size=4, num_patches=8):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.transformer = Transformer(embedding_dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim) * 0.02)\n",
        "        self.position_encoding = nn.Parameter(\n",
        "            torch.randn(1, num_patches * num_patches + 1, embedding_dim) * 0.02\n",
        "        )\n",
        "        self.patch_projection = nn.Linear(patch_size * patch_size * 3, embedding_dim)\n",
        "        \n",
        "        # A Layernorm and a Linear layer are applied on ViT encoder embeddings\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.LayerNorm(embedding_dim), nn.Linear(embedding_dim, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\" \n",
        "        (1) Splitting images into fixed-size patches; \n",
        "        (2) Linearly embed each image patch, append CLS token; \n",
        "        (3) Add position embeddings;\n",
        "        (4) Feed the resulting sequence of vectors to Transformer encoder.\n",
        "        (5) Extract the embeddings corresponding to the CLS token.\n",
        "        (6) Apply output head to the embeddings to obtain the logits\n",
        "        \"\"\"\n",
        "        # BEGIN YOUR CODE\n",
        "        raise NotImplementedError\n",
        "        # END YOUR CODE\n"
      ],
      "metadata": {
        "id": "gpZeS39N8BPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your implementation\n",
        "model = ClassificationViT(10)\n",
        "model.load_state_dict(test_data['weights']['ClassificationViT'])\n",
        "x = test_data['input']['ClassificationViT.forward']\n",
        "y = model.forward(x)\n",
        "check_error('ClassificationViT.forward', y, test_data['output']['ClassificationViT.forward'])\n",
        "\n",
        "model.load_state_dict(auto_grader_data['weights']['ClassificationViT'])\n",
        "x = auto_grader_data['input']['ClassificationViT.forward']\n",
        "y = model.forward(x)\n",
        "auto_grader_data['output']['ClassificationViT.forward'] = y\n",
        "save_auto_grader_data()"
      ],
      "metadata": {
        "id": "D3-wRAClbpnf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader and Preprocess\n",
        "\n",
        "We use ```torchvision``` to download and prepare images and labels."
      ],
      "metadata": {
        "id": "R1m2Q5dNnZXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='/content/data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='/content/data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "F_EBBMgv-FRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Training ViT \n",
        "\n"
      ],
      "metadata": {
        "id": "FnTEdan3r3_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initilize model (ClassificationViT)\n",
        "model = ClassificationViT(10)\n",
        "# Move model to GPU \n",
        "model.to(torch_device)\n",
        "# Create optimizer for the model\n",
        "\n",
        "# You may want to tune these hyperparameters to get better performance\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, betas=(0.9, 0.95), weight_decay=1e-9)\n",
        "\n",
        "total_steps = 0\n",
        "num_epochs = 10\n",
        "train_logfreq = 100\n",
        "losses = []\n",
        "train_acc = []\n",
        "all_val_acc = []\n",
        "best_val_acc = 0\n",
        "\n",
        "epoch_iterator = trange(num_epochs)\n",
        "for epoch in epoch_iterator:\n",
        "    # Train\n",
        "    data_iterator = tqdm(trainloader)\n",
        "    for x, y in data_iterator:\n",
        "        total_steps += 1\n",
        "        x, y = x.to(torch_device), y.to(torch_device)\n",
        "        logits = model(x)\n",
        "        loss = torch.mean(F.cross_entropy(logits, y))\n",
        "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n",
        "\n",
        "        if total_steps % train_logfreq == 0:\n",
        "            losses.append(loss.item())\n",
        "            train_acc.append(accuracy.item())\n",
        "\n",
        "    # Validation\n",
        "    val_acc = []\n",
        "    model.eval()\n",
        "    for x, y in testloader:\n",
        "        x, y = x.to(torch_device), y.to(torch_device)\n",
        "        with torch.no_grad():\n",
        "          logits = model(x)\n",
        "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
        "        val_acc.append(accuracy.item())\n",
        "    model.train()\n",
        "\n",
        "    all_val_acc.append(np.mean(val_acc))\n",
        "    # Save best model\n",
        "    if np.mean(val_acc) > best_val_acc:\n",
        "        best_val_acc = np.mean(val_acc)\n",
        "\n",
        "    epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('Train Loss')\n",
        "plt.figure()\n",
        "plt.plot(train_acc)\n",
        "plt.title('Train Accuracy')\n",
        "plt.figure()\n",
        "plt.plot(all_val_acc)\n",
        "plt.title('Val Accuracy')"
      ],
      "metadata": {
        "id": "5JyOv6GfCFVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your implementation, your accuracy should be greater than 65%\n",
        "auto_grader_data['output']['vit_acc'] = best_val_acc\n",
        "save_auto_grader_data()\n",
        "check_acc(best_val_acc, threshold=0.65)"
      ],
      "metadata": {
        "id": "OikMO6_LCmik",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked AutoEncoder\n",
        "\n",
        "The second part of this notebook is implementing Masked Autoencoder (MAE).\n",
        "The idea of MAE is masking random patches of the input image and reconstruct the missing pixels. This whole achitecture can be seen in the following figure.\n",
        "![mae](https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png)\n",
        "\n",
        "You will train MAE without labels on CIFAR, aka, self-supervised learning.\n",
        "Then you will use the self-supervised pretrained model for linear classification and finetuning experiments."
      ],
      "metadata": {
        "id": "Ij4IVDMIsB5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Masking and Restore\n",
        "\n",
        "To get started with MAE, you need to implement ```random_masking``` to mask random patches from the input image and ```restore_masked``` to combine reconstructed masked part and unmasked part to restore the image."
      ],
      "metadata": {
        "id": "ufTgPU_Rszsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def index_sequence(x, ids):\n",
        "    \"\"\"Index tensor (x) with indices given by ids\n",
        "    Args:\n",
        "        x: input sequence tensor, can be 2D (batch x length) or 3D (batch x length x feature)\n",
        "        ids: 2D indices (batch x length) for re-indexing the sequence tensor\n",
        "    \"\"\"\n",
        "    if len(x.shape) == 3:\n",
        "        ids = ids.unsqueeze(-1).expand(-1, -1, x.shape[-1])\n",
        "    return torch.take_along_dim(x, ids, dim=1)\n",
        "\n",
        "def random_masking(x, keep_length, ids_shuffle):\n",
        "    \"\"\"Apply random masking on input tensor\n",
        "    Args:\n",
        "        x: input patches (batch x length x feature)\n",
        "        keep_length: length of unmasked patches\n",
        "        ids_shuffle: random indices for shuffling the input sequence \n",
        "    Returns:\n",
        "        kept: unmasked part of x\n",
        "        mask: a 2D (batch x length) mask tensor of 0s and 1s indicated which\n",
        "            part of x is masked out. The value 0 indicates not masked and 1\n",
        "            indicates masked.\n",
        "        ids_restore: indices to restore x. If we take the kept part and masked\n",
        "            part of x, concatentate them together and index it with ids_restore,\n",
        "            we should get x back.\n",
        "\n",
        "    Hint:\n",
        "        ids_shuffle contains the indices used to shuffle the sequence (patches).\n",
        "        You should use the provided index_sequence function to re-index the\n",
        "        sequence, and keep the first keep_length number of patches.\n",
        "    \"\"\"\n",
        "    # BEGIN YOUR CODE\n",
        "    raise NotImplementedError\n",
        "    # END YOUR CODE\n",
        "\n",
        "def restore_masked(kept_x, masked_x, ids_restore):\n",
        "    \"\"\"Restore masked patches\n",
        "    Args:\n",
        "        kept_x: unmasked patches\n",
        "        masked_x: masked patches\n",
        "        ids_restore: indices to restore x\n",
        "    Returns:\n",
        "        restored patches\n",
        "    \"\"\"\n",
        "    # BEGIN YOUR CODE\n",
        "    raise NotImplementedError\n",
        "    # END YOUR CODE"
      ],
      "metadata": {
        "id": "eKkRHTjMUOJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your implementation\n",
        "x, ids_shuffle = test_data['input']['random_masking']\n",
        "kept, mask, ids_restore = random_masking(x, 4, ids_shuffle)\n",
        "kept_t, mask_t, ids_restore_t = test_data['output']['random_masking']\n",
        "check_error('random_masking: kept', kept, kept_t)\n",
        "check_error('random_masking: mask', mask, mask_t)\n",
        "check_error('random_masking: ids_restore', ids_restore, ids_restore_t)\n",
        "\n",
        "x, ids_shuffle = auto_grader_data['input']['random_masking']\n",
        "kept, mask, ids_restore = random_masking(x, 4, ids_shuffle)\n",
        "auto_grader_data['output']['random_masking'] = (kept, mask, ids_restore)\n",
        "save_auto_grader_data()\n",
        "\n",
        "kept_x, masked_x, ids_restore = test_data['input']['restore_masked']\n",
        "restored = restore_masked(kept_x, masked_x, ids_restore)\n",
        "check_error('restore_masked', restored, test_data['output']['restore_masked'])\n",
        "\n",
        "kept_x, masked_x, ids_restore = auto_grader_data['input']['restore_masked']\n",
        "restored = restore_masked(kept_x, masked_x, ids_restore)\n",
        "auto_grader_data['output']['restore_masked'] = (kept, mask, ids_restore)\n",
        "save_auto_grader_data()"
      ],
      "metadata": {
        "id": "QxXqQ6Qk9FLX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedAutoEncoder(nn.Module):\n",
        "    \"\"\"MAE Encoder\n",
        "    Args:\n",
        "        encoder: vit encoder\n",
        "        decoder: vit decoder\n",
        "        encoder_embedding_dim: embedding size of encoder\n",
        "        decoder_embedding_dim: embedding size of decoder\n",
        "        patch_size: image patch size\n",
        "        num_patches: number of patches\n",
        "        mask_ratio: percentage of masked patches\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, encoder_embedding_dim=256, \n",
        "                 decoder_embedding_dim=128, patch_size=4, num_patches=8,\n",
        "                 mask_ratio=0.75):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding_dim = encoder_embedding_dim\n",
        "        self.decoder_embedding_dim = decoder_embedding_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.mask_ratio = mask_ratio\n",
        "\n",
        "        self.masked_length = int(num_patches * num_patches * mask_ratio)\n",
        "        self.keep_length = num_patches * num_patches - self.masked_length\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        self.encoder_input_projection = nn.Linear(patch_size * patch_size * 3, encoder_embedding_dim)\n",
        "        self.decoder_input_projection = nn.Linear(encoder_embedding_dim, decoder_embedding_dim)\n",
        "        self.decoder_output_projection = nn.Linear(decoder_embedding_dim, patch_size * patch_size * 3)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, encoder_embedding_dim) * 0.02)\n",
        "        self.encoder_position_encoding = nn.Parameter(torch.randn(1, num_patches * num_patches, encoder_embedding_dim) * 0.02)\n",
        "        self.decoder_position_encoding = nn.Parameter(torch.randn(1, num_patches * num_patches, decoder_embedding_dim) * 0.02)\n",
        "        self.masked_tokens = nn.Parameter(torch.randn(1, 1, decoder_embedding_dim) * 0.02)\n",
        "\n",
        "    def forward_encoder(self, images, ids_shuffle=None):\n",
        "        \"\"\"Encode input images\n",
        "        You should implement the following steps\n",
        "        (1) patchify images into patches\n",
        "        (2) linear projection\n",
        "        (3) add position encoding\n",
        "        (4) concatenate cls_token and patches embedding and pass it to vit encoder\n",
        "        \"\"\"\n",
        "        batch_size = images.shape[0]\n",
        "        # Generate random shuffling indices\n",
        "        if ids_shuffle is None:\n",
        "            ids_shuffle = torch.argsort(\n",
        "                torch.rand(\n",
        "                    (batch_size, self.num_patches * self.num_patches),\n",
        "                    device=images.device\n",
        "                ),\n",
        "                dim=1\n",
        "            )\n",
        "        # BEGIN YOUR CODE\n",
        "        raise NotImplementedError\n",
        "        # END YOUR CODE\n",
        "\n",
        "    def forward_decoder(self, encoder_embeddings, ids_restore):\n",
        "        \"\"\"Decode encoder embeddings\n",
        "        You should implement the following steps\n",
        "        (1) linear projection of encoder embeddings\n",
        "        (2) restore sequence from masked_patches and encoder predictions\n",
        "        (3) add position encoding\n",
        "        (3) readd/use CLS token and decode using ViT decoder \n",
        "        (4) projection to predict image patches\n",
        "        \"\"\"\n",
        "        # BEGIN YOUR CODE\n",
        "        raise NotImplementedError\n",
        "        # END YOUR CODE\n",
        "\n",
        "    def forward(self, images):\n",
        "        encoder_output, mask, ids_restore = self.forward_encoder(images)\n",
        "        decoder_output = self.forward_decoder(encoder_output, ids_restore)\n",
        "        return decoder_output, mask\n",
        "\n",
        "    def forward_encoder_representation(self, images):\n",
        "        \"\"\"Encode images without applying random masking to get representation\n",
        "        of input images. \n",
        "\n",
        "        You should implement splitting images into patches, readd/use CLS token,\n",
        "        and encoding with ViT encoder.\n",
        "        \"\"\"\n",
        "        # BEGIN YOUR CODE\n",
        "        raise NotImplementedError\n",
        "        # END YOUR CODE\n",
        "        "
      ],
      "metadata": {
        "id": "9ud7I0SXK0mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your implementation\n",
        "model = MaskedAutoEncoder(\n",
        "    Transformer(embedding_dim=256, n_layers=4),\n",
        "    Transformer(embedding_dim=128, n_layers=2),\n",
        ")\n",
        "\n",
        "model.load_state_dict(test_data['weights']['MaskedAutoEncoder'])\n",
        "images, ids_shuffle = test_data['input']['MaskedAutoEncoder.forward_encoder']\n",
        "encoder_embeddings_t, mask_t, ids_restore_t = test_data['output']['MaskedAutoEncoder.forward_encoder']\n",
        "encoder_embeddings, mask, ids_restore = model.forward_encoder(\n",
        "    images, ids_shuffle\n",
        ")\n",
        "\n",
        "check_error(\n",
        "    'MaskedAutoEncoder.forward_encoder: encoder_embeddings',\n",
        "    encoder_embeddings, encoder_embeddings_t\n",
        ")\n",
        "check_error(\n",
        "    'MaskedAutoEncoder.forward_encoder: mask',\n",
        "    mask, mask_t\n",
        ")\n",
        "check_error(\n",
        "    'MaskedAutoEncoder.forward_encoder: ids_restore',\n",
        "    ids_restore, ids_restore_t\n",
        ")\n",
        "\n",
        "encoder_embeddings, ids_restore = test_data['input']['MaskedAutoEncoder.forward_decoder']\n",
        "decoder_output_t = test_data['output']['MaskedAutoEncoder.forward_decoder']\n",
        "decoder_output = model.forward_decoder(encoder_embeddings, ids_restore)\n",
        "check_error(\n",
        "    'MaskedAutoEncoder.forward_decoder',\n",
        "    decoder_output,\n",
        "    decoder_output_t\n",
        ")\n",
        "\n",
        "images = test_data['input']['MaskedAutoEncoder.forward_encoder_representation']\n",
        "encoder_representations_t = test_data['output']['MaskedAutoEncoder.forward_encoder_representation']\n",
        "encoder_representations = model.forward_encoder_representation(images)\n",
        "check_error(\n",
        "    'MaskedAutoEncoder.forward_encoder_representation',\n",
        "    encoder_representations,\n",
        "    encoder_representations_t\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model = MaskedAutoEncoder(\n",
        "    Transformer(embedding_dim=256, n_layers=4),\n",
        "    Transformer(embedding_dim=128, n_layers=2),\n",
        ")\n",
        "\n",
        "model.load_state_dict(auto_grader_data['weights']['MaskedAutoEncoder'])\n",
        "images, ids_shuffle = auto_grader_data['input']['MaskedAutoEncoder.forward_encoder']\n",
        "auto_grader_data['output']['MaskedAutoEncoder.forward_encoder'] = model.forward_encoder(\n",
        "    images, ids_shuffle\n",
        ")\n",
        "\n",
        "encoder_embeddings, ids_restore = auto_grader_data['input']['MaskedAutoEncoder.forward_decoder']\n",
        "auto_grader_data['output']['MaskedAutoEncoder.forward_decoder'] = model.forward_decoder(encoder_embeddings, ids_restore)\n",
        "\n",
        "images = auto_grader_data['input']['MaskedAutoEncoder.forward_encoder_representation']\n",
        "auto_grader_data['output']['MaskedAutoEncoder.forward_encoder_representation'] = model.forward_encoder_representation(images)\n",
        "save_auto_grader_data()\n"
      ],
      "metadata": {
        "id": "obJhGP5vfaBl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Masked Autoencoder"
      ],
      "metadata": {
        "id": "t4KO8ZzAOyJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initilize MAE model\n",
        "model = MaskedAutoEncoder(\n",
        "    Transformer(embedding_dim=256, n_layers=4),\n",
        "    Transformer(embedding_dim=128, n_layers=2),\n",
        ")\n",
        "# Move the model to GPU\n",
        "model.to(torch_device)\n",
        "# Create optimizer\n",
        "\n",
        "# You may want to tune these hyperparameters to get better performance\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=0.05)\n",
        "\n",
        "total_steps = 0\n",
        "num_epochs = 20\n",
        "train_logfreq = 100\n",
        "\n",
        "losses = []\n",
        "\n",
        "epoch_iterator = trange(num_epochs)\n",
        "for epoch in epoch_iterator:\n",
        "    # Train\n",
        "    data_iterator = tqdm(trainloader)\n",
        "    for x, y in data_iterator:\n",
        "        total_steps += 1\n",
        "        x = x.to(torch_device)\n",
        "        image_patches = patchify(x)\n",
        "        predicted_patches, mask = model(x)\n",
        "        loss = torch.sum(torch.mean(torch.square(image_patches - predicted_patches), dim=-1) * mask) / mask.sum()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_iterator.set_postfix(loss=loss.item())\n",
        "        if total_steps % train_logfreq == 0:\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    # Periodically save model\n",
        "    torch.save(model.state_dict(), os.path.join(root_folder, \"mae_pretrained.pt\"))\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('MAE Train Loss')"
      ],
      "metadata": {
        "id": "A3WJB5fLqSbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use pretrained MAE model for classification\n",
        "\n",
        "As ViT has a class token, to adapt to this design, in our MAE pre-training we append an auxiliary dummy token to the encoder input. This token will be treated as the class token for training the classifier in linear probing and fine-tuning."
      ],
      "metadata": {
        "id": "-UV8dPF8Pw4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationMAE(nn.Module):\n",
        "    \"\"\"A linear classifier is trained on self-supervised representations learned by MAE. \n",
        "    Args:\n",
        "        n_classes: number of classes\n",
        "        mae: mae model\n",
        "        embedding_dim: embedding dimension of mae output\n",
        "        detach: if True, only the classification head is updated.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes, mae, embedding_dim=256, detach=False):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.mae = mae\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.LayerNorm(embedding_dim), nn.Linear(embedding_dim, n_classes)\n",
        "        )\n",
        "        \"\"\"\n",
        "        When self.detach=True, use linear classification, when self.detach=False,\n",
        "        use full finetuning.\n",
        "        \"\"\"\n",
        "        self.detach = detach\n",
        "\n",
        "    def forward(self, images):\n",
        "        # BEGIN YOUR CODE\n",
        "        raise NotImplementedError\n",
        "        # END YOUR CODE"
      ],
      "metadata": {
        "id": "QODCykcJS0ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your implementation\n",
        "model = ClassificationMAE(\n",
        "    10,\n",
        "    MaskedAutoEncoder(\n",
        "        Transformer(embedding_dim=256, n_layers=4),\n",
        "        Transformer(embedding_dim=128, n_layers=2),\n",
        "    )\n",
        ")\n",
        "\n",
        "model.load_state_dict(test_data['weights']['ClassificationMAE'])\n",
        "\n",
        "check_error(\n",
        "    'ClassificationMAE.forward',\n",
        "    model(test_data['input']['ClassificationMAE.forward']),\n",
        "    test_data['output']['ClassificationMAE.forward']\n",
        ")\n",
        "\n",
        "model = ClassificationMAE(\n",
        "    10,\n",
        "    MaskedAutoEncoder(\n",
        "        Transformer(embedding_dim=256, n_layers=4),\n",
        "        Transformer(embedding_dim=128, n_layers=2),\n",
        "    )\n",
        ")\n",
        "\n",
        "model.load_state_dict(auto_grader_data['weights']['ClassificationMAE'])\n",
        "auto_grader_data['output']['ClassificationMAE.forward'] = model(\n",
        "    auto_grader_data['input']['ClassificationMAE.forward']\n",
        ")\n",
        "save_auto_grader_data()"
      ],
      "metadata": {
        "id": "Fi07CiNMt-pX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the pretrained MAE model"
      ],
      "metadata": {
        "id": "0gnCcwGlVJ-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae = MaskedAutoEncoder(\n",
        "    Transformer(embedding_dim=256, n_layers=4),\n",
        "    Transformer(embedding_dim=128, n_layers=2),\n",
        ")\n",
        "mae.load_state_dict(torch.load(os.path.join(root_folder, \"mae_pretrained.pt\")))"
      ],
      "metadata": {
        "id": "DdQEIE_yYuJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Classification\n",
        "\n",
        "A linear classifier is trained on self-supervised representations learned by MAE. "
      ],
      "metadata": {
        "id": "AyLwLC6VPz2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initilize classification model; set detach=True to only update the linear classifier. \n",
        "model = ClassificationMAE(10, mae, detach=True)\n",
        "model.to(torch_device)\n",
        "\n",
        "# You may want to tune these hyperparameters to get better performance\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-9)\n",
        "\n",
        "total_steps = 0\n",
        "num_epochs = 20\n",
        "train_logfreq = 100\n",
        "losses = []\n",
        "train_acc = []\n",
        "all_val_acc = []\n",
        "best_val_acc = 0\n",
        "\n",
        "epoch_iterator = trange(num_epochs)\n",
        "for epoch in epoch_iterator:\n",
        "    # Train\n",
        "    data_iterator = tqdm(trainloader)\n",
        "    for x, y in data_iterator:\n",
        "        total_steps += 1\n",
        "        x, y = x.to(torch_device), y.to(torch_device)\n",
        "        logits = model(x)\n",
        "        loss = torch.mean(F.cross_entropy(logits, y))\n",
        "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n",
        "\n",
        "        if total_steps % train_logfreq == 0:\n",
        "            losses.append(loss.item())\n",
        "            train_acc.append(accuracy.item())\n",
        "\n",
        "    # Validation\n",
        "    val_acc = []\n",
        "    model.eval()\n",
        "    for x, y in testloader:\n",
        "        x, y = x.to(torch_device), y.to(torch_device)\n",
        "        with torch.no_grad():\n",
        "          logits = model(x)\n",
        "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
        "        val_acc.append(accuracy.item())\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    all_val_acc.append(np.mean(val_acc))\n",
        "\n",
        "    # Save best model\n",
        "    if np.mean(val_acc) > best_val_acc:\n",
        "        best_val_acc = np.mean(val_acc)\n",
        "\n",
        "    epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('Linear Classification Train Loss')\n",
        "plt.figure()\n",
        "plt.plot(train_acc)\n",
        "plt.title('Linear Classification Train Accuracy')\n",
        "plt.figure()\n",
        "plt.plot(all_val_acc)\n",
        "plt.title('Linear Classification Val Accuracy')"
      ],
      "metadata": {
        "id": "KdPfl0MJrfng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your implementation, your accuracy should be greater than 30%\n",
        "auto_grader_data['output']['mae_linear_acc'] = best_val_acc\n",
        "save_auto_grader_data()\n",
        "check_acc(best_val_acc, threshold=0.30)"
      ],
      "metadata": {
        "id": "RwXyr42cDYr3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Finetuning\n",
        "\n",
        "A linear classifer and the pretrained MAE model are jointly updated."
      ],
      "metadata": {
        "id": "frkTBj1wQo_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initilize classification model; set detach=False to update both the linear classifier and pretrained MAE model.\n",
        "model = ClassificationMAE(10, mae, detach=False)\n",
        "model.to(torch_device)\n",
        "\n",
        "# You may want to tune these hyperparameters to get better performance\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-9)\n",
        "\n",
        "total_steps = 0\n",
        "num_epochs = 20\n",
        "train_logfreq = 100\n",
        "losses = []\n",
        "train_acc = []\n",
        "all_val_acc = []\n",
        "best_val_acc = 0\n",
        "\n",
        "epoch_iterator = trange(num_epochs)\n",
        "for epoch in epoch_iterator:\n",
        "    # Train\n",
        "    data_iterator = tqdm(trainloader)\n",
        "    for x, y in data_iterator:\n",
        "        total_steps += 1\n",
        "        x, y = x.to(torch_device), y.to(torch_device)\n",
        "        logits = model(x)\n",
        "        loss = torch.mean(F.cross_entropy(logits, y))\n",
        "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n",
        "\n",
        "        if total_steps % train_logfreq == 0:\n",
        "            losses.append(loss.item())\n",
        "            train_acc.append(accuracy.item())\n",
        "\n",
        "    # Validation\n",
        "    val_acc = []\n",
        "    model.eval()\n",
        "    for x, y in testloader:\n",
        "        x, y = x.to(torch_device), y.to(torch_device)\n",
        "        with torch.no_grad():\n",
        "          logits = model(x)\n",
        "        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
        "        val_acc.append(accuracy.item())\n",
        "    model.train()\n",
        "\n",
        "    all_val_acc.append(np.mean(val_acc))\n",
        "\n",
        "    # Save best model\n",
        "    if np.mean(val_acc) > best_val_acc:\n",
        "        best_val_acc = np.mean(val_acc)\n",
        "\n",
        "    epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('Finetune Classification Train Loss')\n",
        "plt.figure()\n",
        "plt.plot(train_acc)\n",
        "plt.title('Finetune Classification Train Accuracy')\n",
        "plt.figure()\n",
        "plt.plot(all_val_acc)\n",
        "plt.title('Finetune Classification Val Accuracy')"
      ],
      "metadata": {
        "id": "dGTMdBgci8PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your implementation, your accuracy should be greater than 70%\n",
        "auto_grader_data['output']['mae_finetune_acc'] = best_val_acc\n",
        "save_auto_grader_data()\n",
        "check_acc(best_val_acc, threshold=0.70)"
      ],
      "metadata": {
        "id": "Q9k0Ca7MDSFP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Gradescope submission\n",
        "\n",
        "**NOTE:** change the following path to your ```root_dir``` in the begining.\n",
        "\n",
        "Run the following cell will automatically prepare and download ```hw4_submission.zip```.\n",
        "\n",
        "Upload the downloaded file to Gradescope.\n",
        "The Gradescope will run an autograder on the files you submit. \n",
        "\n",
        "It is very unlikely but still possible that your implementation might fail to pass some test cases due to randomness.\n",
        "If you think your code is correct, you can simply rerun the autograder to check check whether it is really due to randomness."
      ],
      "metadata": {
        "id": "b3i2FR3LQ7hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/cs182_hw4_colab\n",
        "!pwd # make sure we are in the right dir\n",
        "\n",
        "!rm hw4_submission.zip\n",
        "!zip hw4_submission.zip -r *.ipynb autograder.pt\n",
        "\n",
        "from google.colab import files\n",
        "files.download('hw4_submission.zip') "
      ],
      "metadata": {
        "id": "NFzRCAcMBu5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RhpgPtiIzDU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}